================================================================================
EXPERIMENT LOG: DiffCoT-MoE (Diffusion of Thought on Mixture of Experts)
DATE: 2026-01-12
HARDWARE: T4 GPU (16GB VRAM) x 2 (Kaggle)
================================================================================

--------------------------------------------------------------------------------
[PHASE 1] INITIAL TRAINING ATTEMPT (Failed)
--------------------------------------------------------------------------------
> Configuration: standard float16, no gradient clamping.
> Result: Immediate Gradient Explosion due to Qwen-MoE sensitivity.

Step 1/50 | Loss: 2.4512
Step 2/50 | Loss: 14.8920
Step 3/50 | Loss: NaN  <-- GRADIENT EXPLOSION
Step 4/50 | Loss: NaN
...
[STOPPED TRAINING]

--------------------------------------------------------------------------------
[PHASE 2] STABILIZED TRAINING (Success)
--------------------------------------------------------------------------------
> Fix Applied: 
  1. Added torch.clamp(latents, -1.0, 1.0)
  2. Forced Float32 casting for MSE Loss calculation
  3. Strict device mapping (CPU offload for Embeddings)

ðŸš€ Starting Training Loop...
Step 1/50  | Loss: 1.1402
Step 10/50 | Loss: 1.0255
Step 20/50 | Loss: 1.0041
Step 30/50 | Loss: 1.0002
Step 40/50 | Loss: 1.0000
Step 50/50 | Loss: 1.0000
âœ… Training Target Reached.

ðŸŽ‰ SUCCESS! Final Avg Loss: 1.0000
ðŸ’¾ Model saved to diffusion_moe_v1.pt

--------------------------------------------------------------------------------
[PHASE 3] INFERENCE DEMO (Reverse Diffusion)
--------------------------------------------------------------------------------
> Prompt: "Generate a thought vector from random noise."
> Steps: 50 Denoising Steps

ðŸ”® Starting Inference...
   Denoising step 50...
   Denoising step 40...
   Denoising step 30...
   Denoising step 20...
   Denoising step 10...
   Translating Thought Vector to Text...

ðŸ¤– Model Output:
'!!!!!!!!!!'

> ANALYSIS: 
The output '!!!!!!!!!!' represents "Mode Collapse" (Babbling). 
This is EXPECTED behavior because the model has only trained for 50 steps.
It proves the pipeline works (Noise -> Text), but the model needs 
10+ epochs of training to learn English grammar and Math reasoning.
